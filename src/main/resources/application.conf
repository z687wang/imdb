my-app {  routes {
    # If ask takes more time than this to complete the request is failed
    ask-timeout = 5s
  }
}

movielens {
    ml-25m {
        input_dir = "/home/hadoop/ml-25m"
        ratings = "ratings.csv"
        movies = "movies.csv"
        links = "links.csv"
        tags = "tags.csv"
        hdfs = {
            input_dir = "ml-25m"
        }
    }
}

collab_filter {
    training {
        training_set_percentage = 0.9
        rank = 10
        numIterations = 100
    }
    model {
        save_path = "hdfs://127.0.0.1:9000/collab_filter/model"
    }
}

redis {
    server = "localhost"
    port = 6379
}

akka {
  loglevel = debug
  system.name = "imdb"
   actor {
    provider = "cluster"
  }
  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }
  cluster {
    seed-nodes = [
      ]

    # auto downing is NOT safe for production deployments.
    # you may want to use it during development, read more about it in the docs.
    #
    # auto-down-unreachable-after = 10s
  }
}

akka.cluster.distributed-data {
  # Actor name of the Replicator actor, /system/ddataReplicator
  name = ddataReplicator

  # Replicas are running on members tagged with this role.
  # All members are used if undefined or empty.
  role = ""

  # How often the Replicator should send out gossip information
  gossip-interval = 2 s
  
  # How often the subscribers will be notified of changes, if any
  notify-subscribers-interval = 500 ms

  # Logging of data with payload size in bytes larger than
  # this value. Maximum detected size per key is logged once,
  # with an increase threshold of 10%.
  # It can be disabled by setting the property to off.
  log-data-size-exceeding = 10 KiB

  # Maximum number of entries to transfer in one round of gossip exchange when
  # synchronizing the replicas. Next chunk will be transferred in next round of gossip.
  # The actual number of data entries in each Gossip message is dynamically
  # adjusted to not exceed the maximum remote message size (maximum-frame-size).
  max-delta-elements = 500
  
  # The id of the dispatcher to use for Replicator actors.
  # If specified you need to define the settings of the actual dispatcher.
  use-dispatcher = "akka.actor.internal-dispatcher"

  # How often the Replicator checks for pruning of data associated with
  # removed cluster nodes. If this is set to 'off' the pruning feature will
  # be completely disabled.
  pruning-interval = 120 s
  
  # How long time it takes to spread the data to all other replica nodes.
  # This is used when initiating and completing the pruning process of data associated
  # with removed cluster nodes. The time measurement is stopped when any replica is 
  # unreachable, but it's still recommended to configure this with certain margin.
  # It should be in the magnitude of minutes even though typical dissemination time
  # is shorter (grows logarithmic with number of nodes). There is no advantage of 
  # setting this too low. Setting it to large value will delay the pruning process.
  max-pruning-dissemination = 300 s
  
  # The markers of that pruning has been performed for a removed node are kept for this
  # time and thereafter removed. If and old data entry that was never pruned is somehow
  # injected and merged with existing data after this time the value will not be correct.
  # This would be possible (although unlikely) in the case of a long network partition.
  # It should be in the magnitude of hours. For durable data it is configured by 
  # 'akka.cluster.distributed-data.durable.pruning-marker-time-to-live'.
 pruning-marker-time-to-live = 6 h
  
  # Serialized Write and Read messages are cached when they are sent to 
  # several nodes. If no further activity they are removed from the cache
  # after this duration.
  serializer-cache-time-to-live = 10s

  # Update and Get operations are sent to oldest nodes first.
  # This is useful together with Cluster Singleton, which is running on oldest nodes.
  prefer-oldest = off
  
  # Settings for delta-CRDT
  delta-crdt {
    # enable or disable delta-CRDT replication
    enabled = on
    
    # Some complex deltas grow in size for each update and above this
    # threshold such deltas are discarded and sent as full state instead.
    # This is number of elements or similar size hint, not size in bytes.
    max-delta-size = 50
  }
  
  durable {
    # List of keys that are durable. Prefix matching is supported by using * at the
    # end of a key.  
    keys = ["duraable*"]
    
    # The markers of that pruning has been performed for a removed node are kept for this
    # time and thereafter removed. If and old data entry that was never pruned is
    # injected and merged with existing data after this time the value will not be correct.
    # This would be possible if replica with durable data didn't participate in the pruning
    # (e.g. it was shutdown) and later started after this time. A durable replica should not 
    # be stopped for longer time than this duration and if it is joining again after this
    # duration its data should first be manually removed (from the lmdb directory).
    # It should be in the magnitude of days. Note that there is a corresponding setting
    # for non-durable data: 'akka.cluster.distributed-data.pruning-marker-time-to-live'.
    pruning-marker-time-to-live = 10 d
    
    # Fully qualified class name of the durable store actor. It must be a subclass
    # of akka.actor.Actor and handle the protocol defined in 
    # akka.cluster.ddata.DurableStore. The class must have a constructor with 
    # com.typesafe.config.Config parameter.
    store-actor-class = akka.cluster.ddata.LmdbDurableStore
    
    use-dispatcher = akka.cluster.distributed-data.durable.pinned-store
    
    pinned-store {
      executor = thread-pool-executor
      type = PinnedDispatcher
    }
    
    # Config for the LmdbDurableStore
    lmdb {
      # Directory of LMDB file. There are two options:
      # 1. A relative or absolute path to a directory that ends with 'ddata'
      #    the full name of the directory will contain name of the ActorSystem
      #    and its remote port.
      # 2. Otherwise the path is used as is, as a relative or absolute path to
      #    a directory.
      #
      # When running in production you may want to configure this to a specific
      # path (alt 2), since the default directory contains the remote port of the
      # actor system to make the name unique. If using a dynamically assigned 
      # port (0) it will be different each time and the previously stored data 
      # will not be loaded.
      dir = "ddata"
      
      # Size in bytes of the memory mapped file.
      map-size = 500 MiB
      
      # Accumulate changes before storing improves performance with the
      # risk of losing the last writes if the JVM crashes.
      # The interval is by default set to 'off' to write each update immediately.
      # Enabling write behind by specifying a duration, e.g. 200ms, is especially 
      # efficient when performing many writes to the same key, because it is only 
      # the last value for each key that will be serialized and stored.  
      # write-behind-interval = 200 ms
      write-behind-interval = off
    }
  }
  
}

tmdb {
    apiKey = "3d5965dd1fd2903e4ab6854c6003559f"
}

recommendation {
    linkFile = "./src/main/resources/links.csv"
}

application {
    host = 0.0.0.0
    port = 8080
}

delta {
    batch_write {
        batch_size = 50
    }
    save_path = "hdfs://127.0.0.1:9000/spark-warehouse"
    table_names = ["moviedetail", "userratings", "moviegenre", "moviecrew"]
}

database {
  postgre {
    connectionPool = "HikariCP"
    dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
    numThreads = 4

    properties = {
      serverName = "localhost"
      portNumber = "5500"
      databaseName = "imdb"
      user = "postgres"
      password = "postgres"
    }
  }
}

hdfs {
    domain = "127.0.0.1" 
    port = 9000
}

spark {
    appName = "IMDB"
    masterURL = "local[*]"
}

crypto {
    key = "thisisnotakey"
}

akka-http-cors {

  # If enabled, allow generic requests (that are outside the scope of the specification)
  # to pass through the directive. Else, strict CORS filtering is applied and any
  # invalid request will be rejected.
  allow-generic-http-requests = yes

  # Indicates whether the resource supports user credentials.  If enabled, the header
  # `Access-Control-Allow-Credentials` is set in the response, indicating that the
  # actual request can include user credentials. Examples of user credentials are:
  # cookies, HTTP authentication or client-side certificates.
  allow-credentials = yes

  # List of origins that the CORS filter must allow. Can also be set to `*` to allow
  # access to the resource from any origin. Controls the content of the
  # `Access-Control-Allow-Origin` response header: if parameter is `*` and credentials
  # are not allowed, a `*` is set in `Access-Control-Allow-Origin`. Otherwise, the
  # origins given in the `Origin` request header are echoed.
  #
  # Hostname starting with `*.` will match any sub-domain.
  # The scheme and the port are always strictly matched.
  #
  # The actual or preflight request is rejected if any of the origins from the request
  # is not allowed.
  allowed-origins = "*"

  # List of request headers that can be used when making an actual request. Controls
  # the content of the `Access-Control-Allow-Headers` header in a preflight response:
  # if parameter is `*`, the headers from `Access-Control-Request-Headers` are echoed.
  # Otherwise the parameter list is returned as part of the header.
  allowed-headers = "*"

  # List of methods that can be used when making an actual request. The list is
  # returned as part of the `Access-Control-Allow-Methods` preflight response header.
  #
  # The preflight request will be rejected if the `Access-Control-Request-Method`
  # header's method is not part of the list.
  allowed-methods = ["GET", "POST", "HEAD", "OPTIONS"]

  # List of headers (other than simple response headers) that browsers are allowed to access.
  # If not empty, this list is returned as part of the `Access-Control-Expose-Headers`
  # header in the actual response.
  exposed-headers = []

  # When set, the amount of seconds the browser is allowed to cache the results of a preflight request.
  # This value is returned as part of the `Access-Control-Max-Age` preflight response header.
  # If `null`, the header is not added to the preflight response.
  max-age = 1800 seconds
}
